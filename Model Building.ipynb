{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnV9C9upL5HC9Z+USPG34I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjung-stat/Customer-Support-Chat-Intent-Classification/blob/main/Model%20Building.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building a text classification model using BERT, we need to preprocess the dataset. First of all, we do one-hot encoding on our target variable. And after that, we use one of the companions of BERT models which is designed to work alongside BERT models, and its purpose is to transform unprocessed textual inputs into the appropriate input format required by BERT. More details can be found below."
      ],
      "metadata": {
        "id": "LVwJO1k3Qn0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "metadata": {
        "id": "ldQkLp1Sp5MS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed training and testing data\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/df_training_complete.pkl', 'rb') as f:   # Load `df_training_complete` from Google Drive\n",
        "    df_training_complete = pickle.load(f)\n",
        "\n",
        "with open('/content/drive/MyDrive/df_testing_copy.pkl', 'rb') as f:   # Load `df_testing_copy` from Google Drive\n",
        "    df_testing_complete = pickle.load(f)"
      ],
      "metadata": {
        "id": "hb5hqrr_oGd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe28ca2c-e074-4d79-9ba7-3bba9d0946e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "J1zqUDsqPNWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot Encoding"
      ],
      "metadata": {
        "id": "iR7r4DVISo3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training and validation sets and reformat testing set. \n",
        "trainfeatures, validfeatures, trainlabels, validlabels = train_test_split(df_training_complete['text'],df_training_complete['category'], stratify=df_training_complete['category'], test_size=0.2)\n",
        "\n",
        "testfeatures=df_testing_complete.copy()\n",
        "testlabels=testfeatures.pop(\"category\")\n",
        "\n",
        "\n",
        "# One-Hot-Encoding of class-labels\n",
        "binarizer=LabelBinarizer()  \n",
        "\n",
        "trainlabels=binarizer.fit_transform(trainlabels.values)\n",
        "validlabels=binarizer.transform(validlabels.values)\n",
        "testlabels=binarizer.transform(testlabels.values)\n",
        "trainfeatures = pd.DataFrame(trainfeatures)\n",
        "validfeatures = pd.DataFrame(validfeatures)"
      ],
      "metadata": {
        "id": "e6fmkh86oGgj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total number of training examples: \", trainfeatures.shape[0])\n",
        "print(\"Total number of validation examples: \", validfeatures.shape[0])\n",
        "print(\"Total number of testing examples: \", testfeatures.shape[0])"
      ],
      "metadata": {
        "id": "pzs-VVOQoGjA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a13d71c8-0173-4df1-a73c-6872c64e5f5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of training examples:  12733\n",
            "Total number of validation examples:  3184\n",
            "Total number of testing examples:  3080\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow_text"
      ],
      "metadata": {
        "id": "iogh2xqcoGqw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd2b2e6-be99-446d-837d-a9407cf72203"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from sklearn.preprocessing import LabelBinarizer"
      ],
      "metadata": {
        "id": "SYkZje5BoGtJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that the following preprocessing model cannot take pandas dataframe as input\n",
        "# Need to convert the input data (text) into list\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\") # The text data we will be working with will undergo preprocessing using a TensorFlow model.\n",
        "                                                                                              # As this preprocessor is a TensorFlow model, it can be easily integrated directly into your own model."
      ],
      "metadata": {
        "id": "tzk1kqUFoGwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ac757c-b94b-4b67-ebed-317ea80db31a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q3yY9yt3SUuu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfSgMy7DSUxC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building"
      ],
      "metadata": {
        "id": "7XEdmyl2Vsli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this intent classification project, we use BERT which is a pretrained language model. BERT is useful for intent classification because it is pre-trained on a large corpus of text data, allowing it to understand the nuances of natural language. Additionally, BERT uses a bidirectional approach, which means it can analyze a text input in both directions, allowing it to better understand the context and meaning of the input. By using BERT as a pre-processing step, you can improve the accuracy and efficiency of your intent classification model."
      ],
      "metadata": {
        "id": "sztDJsvFVzM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1\")"
      ],
      "metadata": {
        "id": "-Tkha_rxSUzk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intent_classification_bert():\n",
        "  \n",
        "  # Initializing the BERT layers\n",
        "  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # define an input tensor --> the input data to the model will be a string of variable length\n",
        "  text_preprocessed = bert_preprocess(input_text) # This layer converts the input string into a format that can be understood by the BERT model\n",
        "  output_bert = bert_model(text_preprocessed) # This layer encodes the input text using the BERT model and returns the encoded outputs\n",
        "                                              # After that, this will be fed into the neural network layers.\n",
        "\n",
        "  # Initializing the neural network layers\n",
        "  encoded_text = output_bert['pooled_output']\n",
        "  layer_1 = tf.keras.layers.Dense(512, activation='relu')(encoded_text)\n",
        "  layer_2 = tf.keras.layers.Dense(256, activation='relu')(layer_1)\n",
        "  layer_3 = tf.keras.layers.Dense(128, activation='relu')(layer_2)\n",
        "  layer_4 = tf.keras.layers.Dropout(0.1)(layer_3) # This layer will be used to prevent model overfitting\n",
        "                                                                                 # We will use 0.1% of the neurons to handle overfitting\n",
        "  output = tf.keras.layers.Dense(trainlabels.shape[1], activation='softmax')(layer_4)  # It only has one neuron. We also initialize the activation function as sigmoid. \n",
        "                                                                                 # sigmoid is used when we have output values that between 0 and 1. \n",
        "                                                                                 # In our case, when making predictions, \n",
        "                                                                                 # the prediction probability will lie between 0 and 1. That’s why it is best suited.\n",
        "                                                                                 # We also name the layer as output because this is our output layer.\n",
        "  return tf.keras.Model(input_text, output)"
      ],
      "metadata": {
        "id": "OUN8D3SiSU2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model = intent_classification_bert()\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) # Since this is a non-binary classification problem and the model outputs probabilities, \n",
        "                                                                 # you’ll use losses.CategoricalCrossentropy loss function.\n",
        "metrics = tf.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOCI10Y6SU4S",
        "outputId": "914e5ff2-f105-442b-86db-9450edac290a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "optimizer=tf.keras.optimizers.Adam(1e-5)\n",
        "classifier_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "zEcVTpzZSU63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = classifier_model.fit(x=trainfeatures, y=trainlabels,\n",
        "                               validation_data=(validfeatures,validlabels),\n",
        "                               batch_size=32,\n",
        "                               #class_weight=class_weights_dict,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpKPOFsDSU9H",
        "outputId": "01b54abc-dd06-42e5-9680-6b013123cf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "398/398 [==============================] - ETA: 0s - loss: 2.5312 - categorical_accuracy: 0.1350"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r398/398 [==============================] - 99s 219ms/step - loss: 2.5312 - categorical_accuracy: 0.1350 - val_loss: 2.4293 - val_categorical_accuracy: 0.2220\n",
            "Epoch 2/20\n",
            "398/398 [==============================] - 87s 218ms/step - loss: 2.3493 - categorical_accuracy: 0.2544 - val_loss: 2.2450 - val_categorical_accuracy: 0.3194\n",
            "Epoch 3/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 2.1798 - categorical_accuracy: 0.3273 - val_loss: 2.0829 - val_categorical_accuracy: 0.3728\n",
            "Epoch 4/20\n",
            "398/398 [==============================] - 82s 206ms/step - loss: 2.0371 - categorical_accuracy: 0.3653 - val_loss: 1.9543 - val_categorical_accuracy: 0.4130\n",
            "Epoch 5/20\n",
            "398/398 [==============================] - 87s 218ms/step - loss: 1.9231 - categorical_accuracy: 0.4022 - val_loss: 1.8528 - val_categorical_accuracy: 0.4400\n",
            "Epoch 6/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.8315 - categorical_accuracy: 0.4286 - val_loss: 1.7696 - val_categorical_accuracy: 0.4614\n",
            "Epoch 7/20\n",
            "398/398 [==============================] - 87s 220ms/step - loss: 1.7533 - categorical_accuracy: 0.4540 - val_loss: 1.6989 - val_categorical_accuracy: 0.4843\n",
            "Epoch 8/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.6893 - categorical_accuracy: 0.4766 - val_loss: 1.6381 - val_categorical_accuracy: 0.5025\n",
            "Epoch 9/20\n",
            "398/398 [==============================] - 87s 217ms/step - loss: 1.6306 - categorical_accuracy: 0.4912 - val_loss: 1.5833 - val_categorical_accuracy: 0.5229\n",
            "Epoch 10/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.5839 - categorical_accuracy: 0.5054 - val_loss: 1.5397 - val_categorical_accuracy: 0.5292\n",
            "Epoch 11/20\n",
            "398/398 [==============================] - 86s 215ms/step - loss: 1.5430 - categorical_accuracy: 0.5193 - val_loss: 1.5012 - val_categorical_accuracy: 0.5349\n",
            "Epoch 12/20\n",
            "398/398 [==============================] - 81s 204ms/step - loss: 1.5006 - categorical_accuracy: 0.5319 - val_loss: 1.4654 - val_categorical_accuracy: 0.5540\n",
            "Epoch 13/20\n",
            "398/398 [==============================] - 85s 213ms/step - loss: 1.4675 - categorical_accuracy: 0.5410 - val_loss: 1.4320 - val_categorical_accuracy: 0.5625\n",
            "Epoch 14/20\n",
            "398/398 [==============================] - 87s 219ms/step - loss: 1.4385 - categorical_accuracy: 0.5490 - val_loss: 1.4095 - val_categorical_accuracy: 0.5669\n",
            "Epoch 15/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.4036 - categorical_accuracy: 0.5622 - val_loss: 1.3770 - val_categorical_accuracy: 0.5810\n",
            "Epoch 16/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 1.3851 - categorical_accuracy: 0.5667 - val_loss: 1.3545 - val_categorical_accuracy: 0.5823\n",
            "Epoch 17/20\n",
            "398/398 [==============================] - 80s 201ms/step - loss: 1.3565 - categorical_accuracy: 0.5804 - val_loss: 1.3331 - val_categorical_accuracy: 0.5864\n",
            "Epoch 18/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.3306 - categorical_accuracy: 0.5844 - val_loss: 1.3143 - val_categorical_accuracy: 0.5886\n",
            "Epoch 19/20\n",
            "398/398 [==============================] - 85s 215ms/step - loss: 1.3102 - categorical_accuracy: 0.5867 - val_loss: 1.2982 - val_categorical_accuracy: 0.5930\n",
            "Epoch 20/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.2881 - categorical_accuracy: 0.5946 - val_loss: 1.2769 - val_categorical_accuracy: 0.6077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_YejhDbid3vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8DmSmaH0d30G"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model building version 2"
      ],
      "metadata": {
        "id": "PDukoofJd32l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intent_classification_bert_v2():\n",
        "  \n",
        "  # Initializing the BERT layers\n",
        "  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # define an input tensor --> the input data to the model will be a string of variable length\n",
        "  text_preprocessed = bert_preprocess(input_text) # This layer converts the input string into a format that can be understood by the BERT model\n",
        "  output_bert = bert_model(text_preprocessed) # This layer encodes the input text using the BERT model and returns the encoded outputs\n",
        "                                              # After that, this will be fed into the neural network layers.\n",
        "\n",
        "  # Initializing the neural network layers\n",
        "  encoded_text = output_bert['pooled_output']\n",
        "  layer_1 = tf.keras.layers.Dense(512, activation='relu')(encoded_text)\n",
        "  layer_2 = tf.keras.layers.Dense(256, activation='relu')(layer_1)\n",
        "  layer_3 = tf.keras.layers.Dense(128, activation='relu')(layer_2)\n",
        "  #layer_4 = tf.keras.layers.Dropout(0.1)(layer_3) # This layer will be used to prevent model overfitting\n",
        "                                                                                 # We will use 0.1% of the neurons to handle overfitting\n",
        "  output = tf.keras.layers.Dense(trainlabels.shape[1], activation='softmax')(layer_3)  # It only has one neuron. We also initialize the activation function as sigmoid. \n",
        "                                                                                 # sigmoid is used when we have output values that between 0 and 1. \n",
        "                                                                                 # In our case, when making predictions, \n",
        "                                                                                 # the prediction probability will lie between 0 and 1. That’s why it is best suited.\n",
        "                                                                                 # We also name the layer as output because this is our output layer.\n",
        "  return tf.keras.Model(input_text, output)"
      ],
      "metadata": {
        "id": "eQQfPAj7bDtQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model_v2 = intent_classification_bert_v2()\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) # Since this is a non-binary classification problem and the model outputs probabilities, \n",
        "                                                                 # you’ll use losses.CategoricalCrossentropy loss function.\n",
        "metrics = tf.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZbNxyakbDv-",
        "outputId": "976488d5-932e-4d54-eaf6-4f8385516303"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "optimizer=tf.keras.optimizers.Adam(1e-5)\n",
        "classifier_model_v2.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "IBNkYQzBbDy2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_v2 = classifier_model_v2.fit(x=trainfeatures, y=trainlabels,\n",
        "                               validation_data=(validfeatures,validlabels),\n",
        "                               batch_size=32,\n",
        "                               #class_weight=class_weights_dict,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uTbxVWXbD1d",
        "outputId": "11d53cbb-2398-4215-9ba3-fd3e1d4fc39d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "398/398 [==============================] - ETA: 0s - loss: 2.5257 - categorical_accuracy: 0.1450"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r398/398 [==============================] - 99s 217ms/step - loss: 2.5257 - categorical_accuracy: 0.1450 - val_loss: 2.4215 - val_categorical_accuracy: 0.2491\n",
            "Epoch 2/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 2.3425 - categorical_accuracy: 0.2871 - val_loss: 2.2709 - val_categorical_accuracy: 0.3156\n",
            "Epoch 3/20\n",
            "398/398 [==============================] - 81s 204ms/step - loss: 2.1826 - categorical_accuracy: 0.3409 - val_loss: 2.1214 - val_categorical_accuracy: 0.3489\n",
            "Epoch 4/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 2.0355 - categorical_accuracy: 0.3835 - val_loss: 1.9933 - val_categorical_accuracy: 0.3847\n",
            "Epoch 5/20\n",
            "398/398 [==============================] - 85s 213ms/step - loss: 1.9142 - categorical_accuracy: 0.4184 - val_loss: 1.8861 - val_categorical_accuracy: 0.4196\n",
            "Epoch 6/20\n",
            "398/398 [==============================] - 86s 215ms/step - loss: 1.8117 - categorical_accuracy: 0.4471 - val_loss: 1.7969 - val_categorical_accuracy: 0.4432\n",
            "Epoch 7/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 1.7273 - categorical_accuracy: 0.4748 - val_loss: 1.7200 - val_categorical_accuracy: 0.4677\n",
            "Epoch 8/20\n",
            "398/398 [==============================] - 80s 202ms/step - loss: 1.6533 - categorical_accuracy: 0.4983 - val_loss: 1.6546 - val_categorical_accuracy: 0.4843\n",
            "Epoch 9/20\n",
            "398/398 [==============================] - 85s 213ms/step - loss: 1.5909 - categorical_accuracy: 0.5158 - val_loss: 1.5966 - val_categorical_accuracy: 0.5072\n",
            "Epoch 10/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 1.5372 - categorical_accuracy: 0.5293 - val_loss: 1.5470 - val_categorical_accuracy: 0.5217\n",
            "Epoch 11/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 1.4908 - categorical_accuracy: 0.5434 - val_loss: 1.5035 - val_categorical_accuracy: 0.5289\n",
            "Epoch 12/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 1.4501 - categorical_accuracy: 0.5545 - val_loss: 1.4655 - val_categorical_accuracy: 0.5437\n",
            "Epoch 13/20\n",
            "398/398 [==============================] - 86s 215ms/step - loss: 1.4134 - categorical_accuracy: 0.5672 - val_loss: 1.4314 - val_categorical_accuracy: 0.5540\n",
            "Epoch 14/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.3816 - categorical_accuracy: 0.5773 - val_loss: 1.3981 - val_categorical_accuracy: 0.5663\n",
            "Epoch 15/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.3511 - categorical_accuracy: 0.5815 - val_loss: 1.3727 - val_categorical_accuracy: 0.5744\n",
            "Epoch 16/20\n",
            "398/398 [==============================] - 86s 217ms/step - loss: 1.3242 - categorical_accuracy: 0.5904 - val_loss: 1.3487 - val_categorical_accuracy: 0.5769\n",
            "Epoch 17/20\n",
            "398/398 [==============================] - 85s 215ms/step - loss: 1.3002 - categorical_accuracy: 0.5992 - val_loss: 1.3270 - val_categorical_accuracy: 0.5835\n",
            "Epoch 18/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.2761 - categorical_accuracy: 0.6057 - val_loss: 1.3071 - val_categorical_accuracy: 0.5911\n",
            "Epoch 19/20\n",
            "398/398 [==============================] - 80s 201ms/step - loss: 1.2555 - categorical_accuracy: 0.6104 - val_loss: 1.2858 - val_categorical_accuracy: 0.5964\n",
            "Epoch 20/20\n",
            "398/398 [==============================] - 85s 213ms/step - loss: 1.2358 - categorical_accuracy: 0.6189 - val_loss: 1.2669 - val_categorical_accuracy: 0.6052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxdxVgIlbD4A"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Building #3"
      ],
      "metadata": {
        "id": "15LfbQqeiOv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intent_classification_bert_v3():\n",
        "  \n",
        "  # Initializing the BERT layers\n",
        "  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # define an input tensor --> the input data to the model will be a string of variable length\n",
        "  text_preprocessed = bert_preprocess(input_text) # This layer converts the input string into a format that can be understood by the BERT model\n",
        "  output_bert = bert_model(text_preprocessed) # This layer encodes the input text using the BERT model and returns the encoded outputs\n",
        "                                              # After that, this will be fed into the neural network layers.\n",
        "\n",
        "  # Initializing the neural network layers\n",
        "  encoded_text = output_bert['pooled_output']\n",
        "  layer_1 = tf.keras.layers.Dense(512, activation='relu')(encoded_text)\n",
        "  layer_2 = tf.keras.layers.Dense(256, activation='relu')(layer_1)\n",
        "  layer_3 = tf.keras.layers.Dense(128, activation='relu')(layer_2)\n",
        "  layer_4 = tf.keras.layers.Dense(64, activation='relu')(layer_3)\n",
        "\n",
        "  #layer_4 = tf.keras.layers.Dropout(0.1)(layer_3) # This layer will be used to prevent model overfitting\n",
        "                                                                                 # We will use 0.1% of the neurons to handle overfitting\n",
        "  output = tf.keras.layers.Dense(trainlabels.shape[1], activation='softmax')(layer_4)  # It only has one neuron. We also initialize the activation function as sigmoid. \n",
        "                                                                                 # sigmoid is used when we have output values that between 0 and 1. \n",
        "                                                                                 # In our case, when making predictions, \n",
        "                                                                                 # the prediction probability will lie between 0 and 1. That’s why it is best suited.\n",
        "                                                                                 # We also name the layer as output because this is our output layer.\n",
        "  return tf.keras.Model(input_text, output)"
      ],
      "metadata": {
        "id": "7zyRY8p-iOyt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model_v3 = intent_classification_bert_v3()\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) # Since this is a non-binary classification problem and the model outputs probabilities, \n",
        "                                                                 # you’ll use losses.CategoricalCrossentropy loss function.\n",
        "metrics = tf.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "LuhtqZ2viO1Y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "optimizer=tf.keras.optimizers.Adam(1e-5)\n",
        "classifier_model_v3.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "XaoyVXssiO3q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_v3 = classifier_model_v3.fit(x=trainfeatures, y=trainlabels,\n",
        "                               validation_data=(validfeatures,validlabels),\n",
        "                               batch_size=32,\n",
        "                               #class_weight=class_weights_dict,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5LIvCNoiO7N",
        "outputId": "5b005991-6334-41d2-de32-d403a3a6dce5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "398/398 [==============================] - ETA: 0s - loss: 2.4979 - categorical_accuracy: 0.1565"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/keras/backend.py:5534: UserWarning: \"`categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r398/398 [==============================] - 90s 217ms/step - loss: 2.4979 - categorical_accuracy: 0.1565 - val_loss: 2.4244 - val_categorical_accuracy: 0.2236\n",
            "Epoch 2/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 2.3365 - categorical_accuracy: 0.2807 - val_loss: 2.2595 - val_categorical_accuracy: 0.3106\n",
            "Epoch 3/20\n",
            "398/398 [==============================] - 86s 216ms/step - loss: 2.1668 - categorical_accuracy: 0.3441 - val_loss: 2.1040 - val_categorical_accuracy: 0.3640\n",
            "Epoch 4/20\n",
            "398/398 [==============================] - 80s 201ms/step - loss: 2.0148 - categorical_accuracy: 0.3847 - val_loss: 1.9641 - val_categorical_accuracy: 0.3901\n",
            "Epoch 5/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.8863 - categorical_accuracy: 0.4224 - val_loss: 1.8569 - val_categorical_accuracy: 0.4215\n",
            "Epoch 6/20\n",
            "398/398 [==============================] - 85s 215ms/step - loss: 1.7844 - categorical_accuracy: 0.4509 - val_loss: 1.7713 - val_categorical_accuracy: 0.4438\n",
            "Epoch 7/20\n",
            "398/398 [==============================] - 84s 212ms/step - loss: 1.7017 - categorical_accuracy: 0.4764 - val_loss: 1.6989 - val_categorical_accuracy: 0.4664\n",
            "Epoch 8/20\n",
            "398/398 [==============================] - 86s 215ms/step - loss: 1.6352 - categorical_accuracy: 0.4947 - val_loss: 1.6365 - val_categorical_accuracy: 0.4934\n",
            "Epoch 9/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.5773 - categorical_accuracy: 0.5121 - val_loss: 1.5878 - val_categorical_accuracy: 0.5022\n",
            "Epoch 10/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.5305 - categorical_accuracy: 0.5260 - val_loss: 1.5434 - val_categorical_accuracy: 0.5101\n",
            "Epoch 11/20\n",
            "398/398 [==============================] - 84s 211ms/step - loss: 1.4897 - categorical_accuracy: 0.5345 - val_loss: 1.5091 - val_categorical_accuracy: 0.5204\n",
            "Epoch 12/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.4542 - categorical_accuracy: 0.5458 - val_loss: 1.4728 - val_categorical_accuracy: 0.5361\n",
            "Epoch 13/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.4202 - categorical_accuracy: 0.5582 - val_loss: 1.4449 - val_categorical_accuracy: 0.5471\n",
            "Epoch 14/20\n",
            "398/398 [==============================] - 85s 215ms/step - loss: 1.3900 - categorical_accuracy: 0.5662 - val_loss: 1.4150 - val_categorical_accuracy: 0.5600\n",
            "Epoch 15/20\n",
            "398/398 [==============================] - 84s 212ms/step - loss: 1.3632 - categorical_accuracy: 0.5739 - val_loss: 1.3871 - val_categorical_accuracy: 0.5675\n",
            "Epoch 16/20\n",
            "398/398 [==============================] - 85s 213ms/step - loss: 1.3392 - categorical_accuracy: 0.5806 - val_loss: 1.3648 - val_categorical_accuracy: 0.5726\n",
            "Epoch 17/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.3142 - categorical_accuracy: 0.5896 - val_loss: 1.3420 - val_categorical_accuracy: 0.5826\n",
            "Epoch 18/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.2932 - categorical_accuracy: 0.5960 - val_loss: 1.3220 - val_categorical_accuracy: 0.5886\n",
            "Epoch 19/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.2724 - categorical_accuracy: 0.6030 - val_loss: 1.2973 - val_categorical_accuracy: 0.5999\n",
            "Epoch 20/20\n",
            "398/398 [==============================] - 85s 214ms/step - loss: 1.2515 - categorical_accuracy: 0.6082 - val_loss: 1.2843 - val_categorical_accuracy: 0.5970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJY9MzX_bD7a"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ybz6rJJAq_mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_XSSnAdUq_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPooling1D, concatenate\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "qz2xtXMGq_sM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intent_classification_bert_v4():\n",
        "  \n",
        "  # Initializing the BERT layers\n",
        "  input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # define an input tensor --> the input data to the model will be a string of variable length\n",
        "  text_preprocessed = bert_preprocess(input_text) # This layer converts the input string into a format that can be understood by the BERT model\n",
        "  output_bert = bert_model(text_preprocessed) # This layer encodes the input text using the BERT model and returns the encoded outputs\n",
        "                                              # After that, this will be fed into the neural network layers.\n",
        "\n",
        "  # Initializing the neural network layers\n",
        "  encoded_text = output_bert['pooled_output']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  x1 = GlobalMaxPooling1D()(output_bert[\"sequence_output\"])\n",
        "  x2 = GlobalMaxPooling1D()(output_bert[\"pooled_output\"])\n",
        "  x = concatenate([x1, x2])\n",
        "  x = Dense(512, activation=\"relu\")(x)\n",
        "  x = Dense(256, activation=\"relu\")(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "  x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "\n",
        "  #layer_4 = tf.keras.layers.Dropout(0.1)(layer_3) # This layer will be used to prevent model overfitting\n",
        "                                                                                 # We will use 0.1% of the neurons to handle overfitting\n",
        "  output = tf.keras.layers.Dense(trainlabels.shape[1], activation='softmax')(x)  # It only has one neuron. We also initialize the activation function as sigmoid. \n",
        "                                                                                 # sigmoid is used when we have output values that between 0 and 1. \n",
        "                                                                                 # In our case, when making predictions, \n",
        "                                                                                 # the prediction probability will lie between 0 and 1. That’s why it is best suited.\n",
        "                                                                                 # We also name the layer as output because this is our output layer.\n",
        "  return tf.keras.Model(input_text, output)"
      ],
      "metadata": {
        "id": "Mc0hoJVPpD_9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gsm-y4KVsxqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # define an input tensor --> the input data to the model will be a string of variable length\n",
        "text_preprocessed = bert_preprocess(input_text) # This layer converts the input string into a format that can be understood by the BERT model\n",
        "output_bert = bert_model(text_preprocessed) # This layer encodes the input text using the BERT model and returns the encoded outputs\n",
        "                                              # After that, this will be fed into the neural network layers.\n",
        "\n",
        "  # Initializing the neural network layers\n",
        "encoded_text = output_bert['pooled_output']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x1 = GlobalMaxPooling1D()(output_bert[\"sequence_output\"])\n",
        "x2 = GlobalMaxPooling1D()(output_bert[\"pooled_output\"])\n",
        "x = concatenate([x1, x2])\n",
        "x = Dense(512, activation=\"relu\")(x)\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dense(trainlabels.shape[1], activation=\"softmax\")(x)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_text, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "NRZNJJWysxtw",
        "outputId": "8f2c1037-52e3-44de-c50e-61de9e48ad5f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-4435b4a33914>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pooled_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_max_pooling1d_3\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained BERT model from TensorFlow Hub\n",
        "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1\", trainable=True)\n",
        "\n",
        "# Define the input layers\n",
        "input_text = Input(shape=(), dtype=tf.string, name=\"input_text\")\n",
        "preprocessed_text = bert_preprocess(input_text)\n",
        "bert_outputs = bert_model(preprocessed_text)\n",
        "\n",
        "# Add some additional layers\n",
        "x1 = GlobalMaxPooling1D()(bert_outputs[\"sequence_output\"])\n",
        "x2 = GlobalMaxPooling1D()(bert_outputs[\"pooled_output\"])\n",
        "x = concatenate([x1, x2])\n",
        "x = Dense(256, activation=\"relu\")(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(trainlabels.shape[1], activation=\"softmax\")(x)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=input_text, outputs=x)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Load your training data\n",
        "train_texts =  trainfeatures # your training texts\n",
        "train_labels = trainlabels # your training labels\n",
        "batch_size = 32\n",
        "\n",
        "# Convert your labels to one-hot encoding\n",
        "train_labels_one_hot = tf.keras.utils.to_categorical(train_labels, num_classes=trainlabels.shape[1])\n",
        "\n",
        "# Define the training data as a TensorFlow Dataset\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_texts, train_labels_one_hot))\n",
        "train_data = train_data.batch(batch_size)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_data, epochs=40)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "dyWpaSRPt6xy",
        "outputId": "1734de6c-3f24-4b77-8474-36f402a10550"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-6adfa6b823ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Add some additional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pooled_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_max_pooling1d_5\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4s0P5yCpsxwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gn_OknQ8sxzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h772jUVxsx2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_model_v4 = intent_classification_bert_v4()\n",
        "\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) # Since this is a non-binary classification problem and the model outputs probabilities, \n",
        "                                                                 # you’ll use losses.CategoricalCrossentropy loss function.\n",
        "metrics = tf.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "rcH-w8hepECz",
        "outputId": "283a5b43-48bd-42f2-db98-d707b9b08141"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-feb984adfcee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier_model_v4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mintent_classification_bert_v4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Since this is a non-binary classification problem and the model outputs probabilities,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                  \u001b[0;31m# you’ll use losses.CategoricalCrossentropy loss function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCategoricalAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-803b748f3e99>\u001b[0m in \u001b[0;36mintent_classification_bert_v4\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_bert\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pooled_output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    233\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"global_max_pooling1d_1\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=20\n",
        "optimizer=tf.keras.optimizers.Adam(1e-5)\n",
        "classifier_model_v4.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ],
      "metadata": {
        "id": "Hmj4Tid-pEFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_v4 = classifier_model_v4.fit(x=trainfeatures, y=trainlabels,\n",
        "                               validation_data=(validfeatures,validlabels),\n",
        "                               batch_size=32,\n",
        "                               #class_weight=class_weights_dict,\n",
        "                               epochs=epochs)"
      ],
      "metadata": {
        "id": "XksT_OYjpEIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zkWOs8i7pELM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2yE1BcgfpETx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}